import fitz  # PyMuPDF
import pytesseract
from pdf2image import convert_from_bytes
import re
import streamlit as st
from docxtpl import DocxTemplate
import os
import time

# Ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n Tesseract
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"
POPPLER_PATH = "/usr/bin"  # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh tr√™n Linux

st.title("üìú Tr√≠ch xu·∫•t th√¥ng tin th·ª≠a ƒë·∫•t t·ª´ PDF scanner")

def clean_text(text):
    replacements = {
        "m¬∞": "m¬≤",
        "m 2": "m¬≤",
        "l√¥ai": "lo·∫°i",
        "ƒë·ªã·∫°": "ƒë·ªãa",
        "CCCD s√¥": "CCCD s·ªë",
        "GCN:": "Gi·∫•y ch·ª©ng nh·∫≠n:",
        # C√°c l·ªói OCR ph·ªï bi·∫øn v·ªÅ ng√†y th√°ng
        "<t": "1",                  # v√≠ d·ª• <t3 -> 13
        "t3": "13",                 # fallback n·∫øu OCR b·ªè m·∫•t d·∫•u
        "th√°ng .": "th√°ng ",
        "nƒÉm¬≤": "nƒÉm ",
        "nƒÉm:": "nƒÉm ",
        "th√°ng:": "th√°ng ",
        "ng√†y:": "ng√†y ",
        "¬≤": "",                    # lo·∫°i b·ªè k√Ω t·ª± m≈© (th∆∞·ªùng OCR nh·∫ßm)
    }

    for wrong, right in replacements.items():
        text = text.replace(wrong, right)

    # Thay v√¨ xo√° t·∫•t c·∫£ \n, ta chu·∫©n ho√° t·ª´ng d√≤ng
    lines = text.split("\n")
    cleaned_lines = [re.sub(r"\s+", " ", line).strip() for line in lines if line.strip()]
    return "\n".join(cleaned_lines).strip()

def normalize_vietnamese_date(text):
    text = text.lower()
    text = re.sub(r"\s+", " ", text).strip()
    match = re.search(r"ng√†y\s*(\d{1,2})\s*th√°ng\s*(\d{1,2})\s*nƒÉm\s*(\d{4})", text)
    if match:
        day, month, year = match.groups()
        return f"{int(day):02}/{int(month):02}/{year}"
    return ""


# H√†m tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ PDF scan
def extract_text_from_scanned_pdf(pdf_bytes):
    images = convert_from_bytes(pdf_bytes.read(), poppler_path=POPPLER_PATH)
    extracted_text = ""
    for img in images:
        text = pytesseract.image_to_string(img, lang="vie")  # OCR ti·∫øng Vi·ªát
        extracted_text += text + "\n"
    return clean_text(extracted_text)  # √Åp d·ª•ng s·ª≠a l·ªói OCR

def extract_clean_field(text, field_label, stop_labels=None):
    if stop_labels:
        stop_pattern = '|'.join([rf"{re.escape(label)}(?:[:\-])?" for label in stop_labels])
        pattern = rf"{re.escape(field_label)}[:\-]?\s*(.*?)(?=\n\s*(?:{stop_pattern})|\n|$)"
    else:
        # D·ª´ng t·∫°i d·∫•u ch·∫•m n·∫øu ƒë·ª©ng cu·ªëi c√¢u (theo sau l√† xu·ªëng d√≤ng ho·∫∑c k·∫øt th√∫c file)
        pattern = rf"{re.escape(field_label)}[:\-]?\s*(.*?)(?=\.\s*\n|\n|$)"

    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        result = match.group(1).strip()

        # # N·∫øu k·∫øt th√∫c b·∫±ng d·∫•u ngo·∫∑c k√©p l·∫° ho·∫∑c k√Ω t·ª± l·ªói ‚Üí xo√°
        # result = re.sub(r'[‚Äù"\'‚Ä∫¬ª]+$', '', result)
        # result = re.sub(r"\s*[\dƒëa]{1,2}\s*$", "", result)

        return result.strip()
    return ""


def extract_loai_dat(text):
    pattern = r"Lo·∫°i ƒë·∫•t[:\-]?\s*(.*?)(?=\.)"
    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
    if match:
        content = match.group(1).strip()
        return content.strip(";: \n")
    return ""

# def extract_field(text, field_label):
#     pattern = rf"({field_label}[:\-]?\s*[\w\s,/.]+(?:\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*m¬≤?)?)"  # Regex s·ª≠a l·∫°i cho ph√π h·ª£p v·ªõi Lo·∫°i ƒë·∫•t
#     match = re.search(pattern, text, re.IGNORECASE)
#     return match.group(1).strip() if match else ""

def extract_land_info(text):
    text = clean_text(text)

    thua_so = re.search(r"Th·ª≠a ƒë·∫•t s·ªë:\s*(\d+)", text, re.IGNORECASE)
    to_ban_do_so = re.search(r"t·ªù b·∫£n ƒë·ªì s·ªë:\s*(\d+)", text, re.IGNORECASE)
    dien_tich = re.search(r"Di·ªán t√≠ch:\s*([\d.,]+)\s*m¬≤?", text, re.IGNORECASE)

    loai_dat = extract_loai_dat(text)
    hinh_thuc_su_dung = extract_clean_field(text, "H√¨nh th·ª©c s·ª≠ d·ª•ng ƒë·∫•t", ["ƒê·ªãa ch·ªâ", "Th·ªùi h·∫°n"])
    dia_chi = extract_clean_field(text, "ƒê·ªãa ch·ªâ", ["Th·ªùi h·∫°n", "Ngu·ªìn g·ªëc", "T√™n t√†i s·∫£n"])
    thoi_han_su_dung = extract_loai_dat(text)
    nguon_goc_su_dung = extract_clean_field(text, "Ngu·ªìn g·ªëc s·ª≠ d·ª•ng", ["Th·ªùi ƒëi·ªÉm ƒëƒÉng k√Ω", "S·ªë v√†o s·ªï"])
    thoi_diem_dang_ky = extract_clean_field(text, "Th·ªùi ƒëi·ªÉm ƒëƒÉng k√Ω v√†o s·ªï ƒë·ªãa ch√≠nh", ["S·ªë v√†o s·ªï", "Ghi ch√∫"])
    so_vao_so_cap_GCN = extract_clean_field(text, "S·ªë v√†o s·ªï c·∫•p Gi·∫•y ch·ª©ng nh·∫≠n", ["Ghi ch√∫", "Chi nh√°nh"])
    noi_dung = re.search(r"Ghi ch√∫[:\-]?\s*(.*?)(?=\.)", text, re.IGNORECASE | re.DOTALL)

    thoi_diem_dang_ky_GCN_raw = re.search(r"(ng√†y\s*\d{0,2}[\s\S]{0,60}nƒÉm\s*\d{4})", text, re.IGNORECASE)

    context_match = re.search(r"(CHI NH√ÅNH[\s\S]{0,300})", text, re.IGNORECASE)
    so_phat_hanh_GCN = None
    if context_match:
        context_block = context_match.group(1)
        so_phat_hanh_GCN = re.search(r"\b([A-Z]{2}\s*\d{6,})\b", context_block)

    nguoi_su_dung_matches = re.findall(
        r"(?:√îng|B√†):\s*([^\n,]+?),\s*CCCD s·ªë:\s*(\d+)(?:,\s*ƒê·ªãa ch·ªâ:\s*([\s\S]*?))?\.",
        text
    )
    nguoi_su_dung = {}
    for i, (ten, cccd, dia_chi_nguoi) in enumerate(nguoi_su_dung_matches, start=1):
        nguoi_su_dung[f"TenNguoi_{i}"] = ten.strip()
        nguoi_su_dung[f"SoCCCD_{i}"] = cccd.strip()
        nguoi_su_dung[f"DiaChiNguoi_{i}"] = dia_chi_nguoi.strip() if dia_chi_nguoi else ""

    return {
        "SoThua": thua_so.group(1).strip() if thua_so else "",
        "SoToBanDo": to_ban_do_so.group(1).strip() if to_ban_do_so else "",
        "DienTich": dien_tich.group(1).strip() if dien_tich else "",
        "LoaiDat": loai_dat,
        "HinhThucSuDung": hinh_thuc_su_dung,
        "DiaChi": dia_chi,
        "ThoiHanSuDung": thoi_han_su_dung,
        "NguonGocSuDung": nguon_goc_su_dung,
        "ThoiDiemDangKy": thoi_diem_dang_ky,
        "SoPhatHanhGCN": so_phat_hanh_GCN.group(1).strip() if so_phat_hanh_GCN else "",
        "SoVaoSoCapGCN": so_vao_so_cap_GCN,
        "ThoiDiemDangKyGCN": normalize_vietnamese_date(thoi_diem_dang_ky_GCN_raw.group(1)) if thoi_diem_dang_ky_GCN_raw else "",
        "NoiDung": noi_dung.group(1).strip() if noi_dung else ""
    }, nguoi_su_dung

# H√†m ƒëi·ªÅn th√¥ng tin v√†o template DOCX
def fill_template_with_data(template_path, land_info, nguoi_su_dung, new_name=None):
    doc = DocxTemplate(template_path)
    context = {**land_info, **nguoi_su_dung}

    if new_name:
        context["TenNguoi_1"] = new_name.strip()

    doc.render(context)

    # T·∫°o t√™n file ƒë·ªông theo t√™n ng∆∞·ªùi s·ª≠ d·ª•ng
    ten_nguoi = context.get("TenNguoi_1", "nguoi_su_dung").replace(" ", "_")
    output_path = f"GCN_{ten_nguoi}.docx"
    doc.save(output_path)
    return output_path

# Upload file PDF
uploaded_file = st.file_uploader("üìÇ Ch·ªçn file PDF", type=["pdf"])

if uploaded_file:
    text = extract_text_from_scanned_pdf(uploaded_file)
    land_info, nguoi_su_dung = extract_land_info(text)  # Tr√≠ch xu·∫•t th√¥ng tin

    if st.button("üì• Xu·∫•t file DOCX v√† T·∫£i v·ªÅ"):
        with st.spinner("ƒêang xu·∫•t file DOCX..."):
            time.sleep(2)  # Gi·∫£ l·∫≠p th·ªùi gian x·ª≠ l√Ω
            template_path = "template.docx"
            docx_file = fill_template_with_data(template_path, land_info, nguoi_su_dung)

        st.success("Xu·∫•t file th√†nh c√¥ng!")
        
        # Cho ph√©p t·∫£i file DOCX
        with open(docx_file, "rb") as file:
            st.download_button(
                label="T·∫£i file DOCX",
                data=file.read(),
                file_name=docx_file,
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )

    # Hi·ªÉn th·ªã k·∫øt qu·∫£ tr√≠ch xu·∫•t
    st.subheader("üè† Th√¥ng tin th·ª≠a ƒë·∫•t:")
    for key, value in land_info.items():
        st.write(f"**{key}:** {value}")

    # Hi·ªÉn th·ªã th√¥ng tin t·ª´ng ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t
    st.subheader("üë§ Ng∆∞·ªùi s·ª≠ d·ª•ng ƒë·∫•t:")
    for i in range(1, len(nguoi_su_dung) // 3 + 1):
        st.write(f"**Ng∆∞·ªùi {i}:** {nguoi_su_dung.get(f'TenNguoi_{i}', '')}")
        st.write(f"**CCCD:** {nguoi_su_dung.get(f'SoCCCD_{i}', '')}")
        st.write(f"**ƒê·ªãa ch·ªâ:** {nguoi_su_dung.get(f'DiaChiNguoi_{i}', '')}")

    with st.expander("üìÑ VƒÉn b·∫£n tr√≠ch xu·∫•t t·ª´ PDF (OCR)"):
        st.text_area("N·ªôi dung:", text, height=300)